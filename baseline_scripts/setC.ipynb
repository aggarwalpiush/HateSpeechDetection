{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IlqliX9gdv8D",
    "outputId": "f98cb9d3-dad6-4e42-b5fe-99adc6278c4a"
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#####################################################################\n",
    "#                           Set C                                   #\n",
    "#####################################################################\n",
    "# Testing a variety of NN architectures with Embeddings             #\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "pd.set_option('max_colwidth',400)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Bidirectional, Dense, \\\n",
    "    LSTM, Conv1D, MaxPooling1D, Dropout, concatenate, Flatten, add\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import Input, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from utilities.preprocess import Preproccesor\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "sG8-UNmQAyrE",
    "outputId": "8167e2f0-73a4-4c09-8317-7a9a584d97c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9dCSMfw-eUJ6",
    "outputId": "617655d5-5c50-4f04-b5c1-cd086435c528"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piush/Ethos-Hate-Speech-Dataset/ethos/experiments/utilities/preprocess.py:84: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
      "  data = pd.read_csv(\"../ethos_data/Davidson_Dataset_Binary.csv\", delimiter='âˆ«')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method     | F1score Precisi Recall  Accurac Specifi Sensiti\n"
     ]
    }
   ],
   "source": [
    "X, y = Preproccesor.load_data(True)\n",
    "\n",
    "class_names = ['noHateSpeech', 'hateSpeech']\n",
    "f = open(\"../results/setC_replicate.txt\", \"a+\")\n",
    "f.write(\"{:<10} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7} \\n\".format('Method','F1score','Precisi','Recall','Accurac','Specifi','Sensiti'))\n",
    "f.write(\"=========================================================================\\n\")\n",
    "f.close()\n",
    "print (\"{:<10} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7}\".format('Method','F1score','Precisi','Recall','Accurac','Specifi','Sensiti'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'\n",
    "!wget 'http://nlp.stanford.edu/data/glove.42B.300d.zip' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "xMD34FnoeN6H",
    "outputId": "b447bcbc-d273-48ed-b88b-6952904b7809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ZipInfo filename='crawl-300d-2M.vec' compress_type=deflate filemode='-rw-r--r--' file_size=4514687127 compress_size=1523784963>]\n",
      "[<ZipInfo filename='glove.42B.300d.txt' compress_type=deflate filemode='-rw-rw-r--' file_size=5025028820 compress_size=1877800207>]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"crawl-300d-2M.vec.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "    print(zip_ref.filelist)\n",
    "with zipfile.ZipFile(\"glove.42B.300d.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "    print(zip_ref.filelist)\n",
    "\n",
    "del zip_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V3Ub11VQQ6uI"
   },
   "outputs": [],
   "source": [
    "!rm 'crawl-300d-2M.vec.zip'\n",
    "!rm 'glove.42B.300d.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJmfoG1SeYuk"
   },
   "outputs": [],
   "source": [
    "def specificity(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    if(tn+fp)>0:\n",
    "        speci = tn/(tn+fp)\n",
    "        return speci\n",
    "    return 0\n",
    "def sensitivity(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    if(tp+fn)>0:\n",
    "        sensi = tp/(tp+fn)\n",
    "        return sensi\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3cVmRJwe4gE"
   },
   "outputs": [],
   "source": [
    "embedding_path1 = \"crawl-300d-2M.vec\" #FastText\n",
    "embedding_path2 = \"glove.42B.300d.txt\" #Glove 300d\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGSO0Vv1fcN4"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SM-R5Y-7f10f"
   },
   "outputs": [],
   "source": [
    "def build_matrix(embedding_path, tk, max_features):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding = \"utf-8\"))\n",
    "\n",
    "    word_index = tk.word_index\n",
    "    nb_words = max_features\n",
    "    embedding_matrix = np.zeros((nb_words + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "def create_embedding_matrix(embed, tk, max_features):\n",
    "    if embed == 1:\n",
    "      return build_matrix(embedding_path1, tk, max_features)\n",
    "    elif embed == 2:\n",
    "      return build_matrix(embedding_path2, tk, max_features)\n",
    "    else:\n",
    "      return np.concatenate([build_matrix(embedding_path1, tk, max_features), build_matrix(embedding_path2, tk, max_features)], axis=-1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oEX9FDJg7WE"
   },
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRIzXMiphJoq"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'supports_masking' : self.supports_masking,\n",
    "            'init': self.init,\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias': self.bias,\n",
    "            'step_dim': self.step_dim,\n",
    "            'features_dim' : self.features_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                              K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IcsGGos6hJ_0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "def build_model1(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=1, save_best_only=True, mode=\"max\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=patience)\n",
    "    with strategy.scope():\n",
    "        inp = Input(shape=(max_len,))\n",
    "        x = Embedding(max_features + 1, embed_size * 2, weights=[embedding_matrix], trainable=False)(inp)\n",
    "        x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "        att = Attention(max_len)(x1)\n",
    "        x = Conv1D(conv_size, 2, activation='relu', padding='same')(x1)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(conv_size, 3, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = concatenate([x, att])\n",
    "        x = Dropout(dr)(Dense(dense_units, activation='relu')(x))\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=10, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    with strategy.scope():\n",
    "        model2 = Model(inputs=inp, outputs=x)\n",
    "        model2.load_weights(file_path)\n",
    "        model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    return model2\n",
    "def build_model2(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=1,save_best_only=True, mode=\"max\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=patience)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features + 1, embed_size * 2, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Conv1D(200, 10, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(rate=0.35))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model2 = model\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=10, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    model2.load_weights(file_path)\n",
    "    model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    return model2\n",
    "def build_model3(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=1,save_best_only=True, mode=\"max\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=patience)\n",
    "    main_input = Input(shape = (max_len,),name='main_input')\n",
    "    glove_Embed = (Embedding(max_features + 1, embed_size * 2, weights=[embedding_matrix], trainable=False))(main_input)\n",
    "    y = LSTM(300)(glove_Embed)\n",
    "    y = Dense(200, activation='relu')(y)\n",
    "    y = Dropout(rate=0.15)(y)\n",
    "    z = Dense(100, activation='relu')(y)\n",
    "    output_lay = Dense(1, activation='sigmoid')(z)\n",
    "    model = Model(inputs=[main_input], outputs=[output_lay])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    model2 = Model(inputs=[main_input], outputs=[output_lay])\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=10, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    model2.load_weights(file_path)\n",
    "    model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    return model2\n",
    "def build_model4(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=1,save_best_only=True, mode=\"max\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=patience)\n",
    "    main_input = Input(shape=(max_len,), dtype='int32', name='main_input')\n",
    "    glove_Embed = (Embedding(max_features + 1, embed_size * 2, input_length=max_len, weights=[embedding_matrix], trainable=False))(main_input)\n",
    "\n",
    "    x0 = Conv1D(128, 10, activation='relu')(glove_Embed)\n",
    "    x1 = Conv1D(64, 5, activation='relu')(x0)\n",
    "    x2 = Conv1D(32, 4, activation='relu')(x1)\n",
    "    x3 = Conv1D(16, 3, activation='relu')(x2)\n",
    "    x4 = Conv1D(8, 5, activation='relu')(x3)\n",
    "    x = MaxPooling1D(pool_size=3)(x4)\n",
    "    x = Dropout(rate=0.25)(x)\n",
    "    x = LSTM(100)(x)\n",
    "\n",
    "    p = MaxPooling1D(pool_size=10)(x0)\n",
    "    p = Dropout(rate=0.15)(p)\n",
    "    p = LSTM(100)(p)\n",
    "\n",
    "    o = MaxPooling1D(pool_size=8)(x1)\n",
    "    o = Dropout(rate=0.15)(o)\n",
    "    o = LSTM(100)(o)\n",
    "\n",
    "    i = MaxPooling1D(pool_size=6)(x2)\n",
    "    i = Dropout(rate=0.15)(i)\n",
    "    i = LSTM(100)(i)\n",
    "\n",
    "    r = MaxPooling1D(pool_size=4)(x3)\n",
    "    r = Dropout(rate=0.15)(r)\n",
    "    r = LSTM(100)(r)\n",
    "\n",
    "    t = MaxPooling1D(pool_size=3)(x4)\n",
    "    t = Dropout(rate=0.15)(t)\n",
    "    t = LSTM(100)(t)\n",
    "\n",
    "    y = LSTM(500)(glove_Embed)\n",
    "    y = Dense(250,activation='relu')(y)\n",
    "    y = Dropout(rate=0.15)(y)\n",
    "\n",
    "    z = concatenate([x, p, o, i, r, t, y])\n",
    "\n",
    "    z = Dense(400,activation='relu')(z)\n",
    "    z = Dropout(0.15)(z)\n",
    "    z = Dense(200,activation='relu')(z)\n",
    "    z = Dense(100,activation='relu')(z)\n",
    "    z = Dropout(0.15)(z)\n",
    "    z = Dense(50,activation='relu')(z)\n",
    "    output_lay = Dense(1, activation='sigmoid')(z)\n",
    "    model = Model(inputs=[main_input], outputs=[output_lay])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    model2 = Model(inputs=[main_input], outputs=[output_lay])\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=10, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    model2.load_weights(file_path)\n",
    "    model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    return model2\n",
    "def build_model5(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=1,save_best_only=True, mode=\"max\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=patience)\n",
    "    main_input = Input(shape=(max_len,), dtype='int32', name='main_input')\n",
    "    x = (Embedding(max_features + 1, embed_size*2, input_length=max_len, weights=[embedding_matrix], trainable=False))(main_input)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = Dense(1024, activation='relu')(hidden)\n",
    "    hidden = Dense(512, activation='relu')(hidden)\n",
    "    output_lay = Dense(1, activation='sigmoid')(hidden)\n",
    "    model = Model(inputs=[main_input], outputs=[output_lay])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    model2 = Model(inputs=[main_input], outputs=[output_lay])\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=10, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    model2.load_weights(file_path)\n",
    "    model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    return model2\n",
    "def build_model6(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=1,save_best_only=True, mode=\"max\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=patience)\n",
    "    with strategy.scope():\n",
    "        main_input = Input(shape=(max_len,), name='main_input')\n",
    "        x = (Embedding(max_features + 1, embed_size*2, input_length=max_len, weights=[embedding_matrix], trainable=False))(main_input)\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "        hidden = concatenate([\n",
    "            Attention(max_len)(x),\n",
    "            GlobalMaxPooling1D()(x),\n",
    "        ])\n",
    "        hidden = Dense(1024, activation='relu')(hidden)\n",
    "        hidden = Dense(512, activation='relu')(hidden)\n",
    "        output_lay = Dense(1, activation='sigmoid')(hidden)\n",
    "        model = Model(inputs=[main_input], outputs=[output_lay])\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "        model2 = Model(inputs=[main_input], outputs=[output_lay])\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=10, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    with strategy.scope():\n",
    "        model2.load_weights(file_path)\n",
    "        model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    return model2\n",
    "def run_model_on_fold(name, max_len, embed_size, embed, bulid_fun):\n",
    "    max_features = 50000\n",
    "    scores = {}\n",
    "    scores.setdefault('fit_time', [])\n",
    "    scores.setdefault('score_time', [])\n",
    "    scores.setdefault('test_F1', [])\n",
    "    scores.setdefault('test_Precision', [])\n",
    "    scores.setdefault('test_Recall', [])\n",
    "    scores.setdefault('test_Accuracy', [])\n",
    "    scores.setdefault('test_Specificity', [])\n",
    "    scores.setdefault('test_Sensitivity', [])\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        tk = Tokenizer(lower = True, filters='', num_words=max_features, oov_token = True)\n",
    "        tk.fit_on_texts(X_train)\n",
    "        train_tokenized = tk.texts_to_sequences(X_train)\n",
    "        valid_tokenized = tk.texts_to_sequences(X_valid)\n",
    "        X_train = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "        X_valid = pad_sequences(valid_tokenized, maxlen=max_len)\n",
    "        embedding_matrix = create_embedding_matrix(embed, tk, max_features)\n",
    "        \n",
    "        model = bulid_fun(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix,\n",
    "                             lr=1e-3, lr_d=0, spatial_dr=0.1, dense_units=128, conv_size=128, dr=0.1, patience=4,\n",
    "                             fold_id=fold_n)\n",
    "\n",
    "        y_preds = []\n",
    "        for i in model.predict(X_valid):\n",
    "            if i[0] >= 0.5:\n",
    "                y_preds.append(1)\n",
    "            else:\n",
    "                y_preds.append(0)\n",
    "        print(accuracy_score(y_valid, y_preds))\n",
    "        scores['test_F1'].append(f1_score(y_valid, y_preds, average='macro'))\n",
    "        scores['test_Precision'].append(precision_score(y_valid, y_preds, average='macro'))\n",
    "        scores['test_Recall'].append(recall_score(y_valid, y_preds, average='macro'))\n",
    "        scores['test_Accuracy'].append(accuracy_score(y_valid, y_preds))\n",
    "        scores['test_Specificity'].append(specificity(y_valid, y_preds))\n",
    "        scores['test_Sensitivity'].append(sensitivity(y_valid, y_preds))\n",
    "    f = open(\"../results/setC_replicate.txt\", \"a+\")\n",
    "    f.write(\"{:<10} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7}\".format(str(name)[:7],\n",
    "                                                               str('%.4f' % (sum(scores['test_F1']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Precision']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Recall']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Accuracy']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Specificity']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Sensitivity']) / 10)))+'\\n')\n",
    "    f.close()\n",
    "    print(\"{:<10} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7}\".format(str(name)[:7],\n",
    "                                                               str('%.4f' % (sum(scores['test_F1']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Precision']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Recall']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Accuracy']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Specificity']) / 10)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Sensitivity']) / 10))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_load(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0, dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n",
    "    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=1,save_best_only=True, mode=\"max\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=patience)\n",
    "    with strategy.scope():\n",
    "        main_input = Input(shape=(max_len,), name='main_input')\n",
    "        x = (Embedding(max_features + 1, embed_size*2, input_length=max_len, weights=[embedding_matrix], trainable=False))(main_input)\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "        hidden = concatenate([\n",
    "            Attention(max_len)(x),\n",
    "            GlobalMaxPooling1D()(x),\n",
    "        ])\n",
    "        hidden = Dense(1024, activation='relu')(hidden)\n",
    "        hidden = Dense(512, activation='relu')(hidden)\n",
    "        output_lay = Dense(1, activation='sigmoid')(hidden)\n",
    "        model = Model(inputs=[main_input], outputs=[output_lay])\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "        model2 = Model(inputs=[main_input], outputs=[output_lay])\n",
    "    #model.fit(X_train, y_train, batch_size=16, epochs=10, validation_data=(X_valid, y_valid), verbose=1, callbacks=[early_stop, check_point])\n",
    "    with strategy.scope():\n",
    "        model2.load_weights(file_path)\n",
    "        model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    return model2\n",
    "def run_model_on_fold_load(name, max_len, embed_size, embed, bulid_fun):\n",
    "    max_features = 50000\n",
    "    scores = {}\n",
    "    scores.setdefault('fit_time', [])\n",
    "    scores.setdefault('score_time', [])\n",
    "    scores.setdefault('test_F1', [])\n",
    "    scores.setdefault('test_Precision', [])\n",
    "    scores.setdefault('test_Recall', [])\n",
    "    scores.setdefault('test_Accuracy', [])\n",
    "    scores.setdefault('test_Specificity', [])\n",
    "    scores.setdefault('test_Sensitivity', [])\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        if fold_n > 2:\n",
    "            break\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        tk = Tokenizer(lower = True, filters='', num_words=max_features, oov_token = True)\n",
    "        tk.fit_on_texts(X_train)\n",
    "        train_tokenized = tk.texts_to_sequences(X_train)\n",
    "        valid_tokenized = tk.texts_to_sequences(X_valid)\n",
    "        X_train = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "        X_valid = pad_sequences(valid_tokenized, maxlen=max_len)\n",
    "        embedding_matrix = create_embedding_matrix(embed, tk, max_features)\n",
    "        \n",
    "        model = bulid_fun(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix,\n",
    "                             lr=1e-3, lr_d=0, spatial_dr=0.1, dense_units=128, conv_size=128, dr=0.1, patience=4,\n",
    "                             fold_id=fold_n)\n",
    "\n",
    "        y_preds = []\n",
    "        for i in model.predict(X_valid):\n",
    "            if i[0] >= 0.5:\n",
    "                y_preds.append(1)\n",
    "            else:\n",
    "                y_preds.append(0)\n",
    "        print(accuracy_score(y_valid, y_preds))\n",
    "        scores['test_F1'].append(f1_score(y_valid, y_preds, average='macro'))\n",
    "        scores['test_Precision'].append(precision_score(y_valid, y_preds, average='macro'))\n",
    "        scores['test_Recall'].append(recall_score(y_valid, y_preds, average='macro'))\n",
    "        scores['test_Accuracy'].append(accuracy_score(y_valid, y_preds))\n",
    "        scores['test_Specificity'].append(specificity(y_valid, y_preds))\n",
    "        scores['test_Sensitivity'].append(sensitivity(y_valid, y_preds))\n",
    "\n",
    "    f = open(\"../results/setC_replicate.txt\", \"a+\")\n",
    "    f.write(\"{:<10} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7}\".format(str(name)[:7],\n",
    "                                                               str('%.4f' % (sum(scores['test_F1']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Precision']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Recall']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Accuracy']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Specificity']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Sensitivity']) / 3)))+'\\n')\n",
    "    f.close()\n",
    "    print(\"{:<10} | {:<7} {:<7} {:<7} {:<7} {:<7} {:<7}\".format(str(name)[:7],\n",
    "                                                               str('%.4f' % (sum(scores['test_F1']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Precision']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Recall']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Accuracy']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Specificity']) / 3)),\n",
    "                                                               str('%.4f' % (sum(scores['test_Sensitivity']) / 3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kw59OxJjATrs",
    "outputId": "9fad3096-4b2e-41df-e885-c36027e53ac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Fri Oct  2 12:43:56 2020\n",
      "0.7676482452601856\n",
      "Fold 1 started at Fri Oct  2 12:47:50 2020\n",
      "0.7712787414279951\n",
      "Fold 2 started at Fri Oct  2 12:51:47 2020\n",
      "0.7587736990722065\n",
      "b6_load    | 0.5477  0.5777  0.5470  0.7659  0.9139  0.1800 \n"
     ]
    }
   ],
   "source": [
    "for emb_ma in [1]:\n",
    "    embed_size = 150 # * 2 = 300 for matrix 1 and 2\n",
    "    if emb_ma == 3:\n",
    "        embed_size = 300\n",
    "    for max_len in [100]: \n",
    "        #run_model_on_fold('b1_'+str(emb_ma)+'_'+str(max_len),max_len,embed_size,emb_ma,build_model1)\n",
    "        #run_model_on_fold('b2_'+str(emb_ma)+'_'+str(max_len),max_len,embed_size,emb_ma,build_model2)\n",
    "        #run_model_on_fold('b3_'+str(emb_ma)+'_'+str(max_len),max_len,embed_size,emb_ma,build_model3)\n",
    "        #run_model_on_fold('b4_'+str(emb_ma)+'_'+str(max_len),max_len,embed_size,emb_ma,build_model4)\n",
    "        #run_model_on_fold('b5_'+str(emb_ma)+'_'+str(max_len),max_len,embed_size,emb_ma,build_model5)\n",
    "        run_model_on_fold_load('b6_load'+str(emb_ma)+'_'+str(max_len),max_len,embed_size,emb_ma,build_model_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RHhhcN2BAKEw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CNN+Class.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
